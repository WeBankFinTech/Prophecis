apiVersion: v1
kind: ConfigMap
metadata:
  name: yarn-resource-setting
  namespace: {{.Values.namespace}}
data:
  config.json: |
        {
          "kernel_python_credentials" : {
            "username": "",
            "password": "",
            "url": "http://127.0.0.1:8998",
            "auth": "None"
          },
          "kernel_scala_credentials" : {
            "username": "",
            "password": "",
            "url": "http://127.0.0.1:8998",
            "auth": "None"
          },
          "kernel_r_credentials": {
            "username": "",
            "password": "",
            "url": "http://127.0.0.1:8998"
          },
          "logging_config": {
            "version": 1,
            "formatters": {
              "magicsFormatter": { 
                "format": "%(asctime)s\t%(levelname)s\t%(message)s",
                "datefmt": ""
              }
            },
            "handlers": {
              "magicsHandler": { 
                "class": "hdijupyterutils.filehandler.MagicsFileHandler",
                "formatter": "magicsFormatter",
                "home_path": "~/.sparkmagic"
              }
            },
            "loggers": {
              "magicsLogger": { 
                "handlers": ["magicsHandler"],
                "level": "DEBUG",
                "propagate": 0
              }
            }
          },
          "wait_for_idle_timeout_seconds": 15,
          "livy_session_startup_timeout_seconds": 60,
          "fatal_error_suggestion": "The code failed because of a fatal error:\n\t{}.\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.",
          "ignore_ssl_errors": false,
          "session_configs": {
            "driverMemory": "",
            "proxyUser": "",
            "queue": "",
            "conf": {
                "spark.executor.cores": "",
                "spark.executor.memory": "",
                "spark.executor.instances": "",
                "spark.jars.packages": "com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1",
                "spark.jars.repositories": "https://mmlspark.azureedge.net/maven",
                "spark.jars.excludes": "org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11,org.scalactic:scalactic_2.11,org.scalatest:scalatest_2.11"
            }
          },
          "use_auto_viz": true,
          "coerce_dataframe": true,
          "max_results_sql": 2500,
          "pyspark_dataframe_encoding": "utf-8",
          "heartbeat_refresh_seconds": 30,
          "livy_server_heartbeat_timeout_seconds": 0,
          "heartbeat_retry_seconds": 10,
          "server_extension_default_kernel_name": "pysparkkernel",
          "custom_headers": {},
          "retry_policy": "configurable",
          "retry_seconds_to_sleep_list": [0.2, 0.5, 1, 3, 5],
          "configurable_retry_policy_max_retries": 8
        }

  linkismagic.json: |
        {
          "base_url":"http://http://127.0.0.1:9001",
          "headers": {
                "Token-User":"hduser05",
                "Content-Type":"application/json"
          },
          "session_configs": {
                "wds.linkis.yarnqueue":"root.dws",
                "spark.driver.memory":"4g",
                "spark.executor.instances":"1",
                "spark.executor.memory":"4g",
                "wds.linkis.instance": 3
          }
        }
