# Default values for tensorrt-inference-serving.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Kubernetes configuration
## support NodePort, LoadBalancer
##
serviceType: ClusterIP

## serving name
servingName:

## expose the service to the grpc client
port: 8500
restApiPort: 8501
replicas: 1

# repository: "cheyang/tf-model-server-gpu"
image: "tensorflow/serving:latest"

imagePullPolicy: "IfNotPresent"

resources: {}
  #  limits:
  #    cpu: 1.0
  #    memory: 512Mi
  #    nvidia.com/gpu: 1
  #  requests:
  #    cpu: 1.0
  #    memory: 512Mi
  #    nvidia.com/gpu: 1


## The command and args to run the pod

## the mount path inside the container
# mountPath: /serving/inception-export




